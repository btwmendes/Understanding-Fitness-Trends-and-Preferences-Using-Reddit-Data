{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in this project was collecting reddit data from reddit.com. Pushshift.io reddit API facilitated the data collection by collecting 100 posts per request and restricting the search to a specific subreddit. Additionally, pushshift has a \"before\" parameter that allowed me to specify the date and time of the desired posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(subreddit, size, before_time=1611893764):\n",
    "   \n",
    "    # The base_url is provided in the pushshift documentation\n",
    "    base_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    # A list to capture each request\n",
    "    posts = [] \n",
    "    posts_length = 0 \n",
    "    \n",
    "    #The while loops runs until the desired number of posts have been accumulated\n",
    "    while posts_length < size:\n",
    "        res = requests.get(base_url, params = {\"subreddit\": subreddit, \"size\": 100, \"before\": before_time}).json()\n",
    "        data = res['data']\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Captures the before time so the next request pulls older posts\n",
    "        before_time = int(df[['created_utc']].sort_values('created_utc').values[0])\n",
    "        posts.append(df)\n",
    "        posts_length += len(data)\n",
    "    \n",
    "        # A print statement to show progress (larger requests will take more time)\n",
    "        if (posts_length) % 2000 == 0:\n",
    "            print(f'Post {posts_length} of {size}')\n",
    "        \n",
    "        # Best practice - slow the number of requests to lessen the demand on the website\n",
    "        time.sleep(3)\n",
    "    \n",
    "    return pd.concat(posts)\n",
    "\n",
    "# Code Help:\n",
    "\n",
    "# push api -- https://github.com/pushshift/api\n",
    "# tutorial --> https://www.youtube.com/watch?v=AcrjEWsMi_E&feature=youtu.be\n",
    "# time --> https://stackoverflow.com/questions/52004801/how-to-slow-down-asynchrounous-api-calls-to-match-api-limits\n",
    "# utc time converter --> https://www.epochconverter.com\n",
    "# web scaping loop --> https://medium.com/better-programming/how-to-scrape-multiple-pages-of-a-website-using-a-python-web-scraper-4e2c641cff8\n",
    "# web scaping loop --> https://levelup.gitconnected.com/make-your-python-web-scraper-smarter-6233f2d10c3f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bodyweight Fitness Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyweight_fitness = get_data('bodyweightfitness', 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyweight_fitness.to_csv('../datasets/bodyweight_fitness', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powerlifting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post 1000 of 10000\n",
      "Post 2000 of 10000\n",
      "Post 3000 of 10000\n",
      "Post 4000 of 10000\n",
      "Post 5000 of 10000\n",
      "Post 6000 of 10000\n",
      "Post 7000 of 10000\n",
      "Post 8000 of 10000\n",
      "Post 9000 of 10000\n",
      "Post 10000 of 10000\n"
     ]
    }
   ],
   "source": [
    "#Gathering data from PS4 subreddit \n",
    "powerlifting = get_data('powerlifting', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerlifting.to_csv('../datasets/powerlifting', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
